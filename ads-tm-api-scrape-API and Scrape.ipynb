{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ebdb43f3130844a1aeee2363d226f4b0",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# ADS 509 Module 1: APIs and Web Scraping\n",
    "\n",
    "This notebook has three parts. In the first part you will pull data from the Twitter API. In the second, you will scrape lyrics from AZLyrics.com. In the last part, you'll run code that verifies the completeness of your data pull. \n",
    "\n",
    "For this assignment you have chosen two musical artists who have at least 100,000 Twitter followers and 20 songs with lyrics on AZLyrics.com. In this part of the assignment we pull the some of the user information for the followers of your artist and store them in text files. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b8377b58760744c4a3363726a38e73af",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Important Note\n",
    "\n",
    "This assignment requires you to have a version of Tweepy that is at least version 4. The latest version is 4.10 as I write this. Critically, this version of Tweepy is *not* on the upgrade path from Version 3, so you will not be able to simply upgrade the package if you are on Version 3. Instead you will need to explicitly install version 4, which you can do with a command like this: `pip install \"tweepy>=4\"`. You will also be using Version 2 of the Twitter API for this assignment. \n",
    "\n",
    "Run the below cell. If your version of Tweepy begins with a \"4\", then you should be good to go. If it begins with a \"3\" then run the following command, found [here](https://stackoverflow.com/questions/5226311/installing-specific-package-version-with-pip), at the command line or in a cell: `pip install -Iv tweepy==4.9`. (You may want to update that version number if Tweepy has moved on past 4.9. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "fbe2a6d5566f41bfa1a87207e0d00157",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it. \n",
    "\n",
    "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link. \n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell. \n",
    "\n",
    "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. *Make sure to answer every question marked with a `Q:` for full credit.* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b523a958f9494f718d388e246d3848cb",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Twitter API Pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "c66c7971a1074c00b89399ff25728f91",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4854,
    "execution_start": 1673845650475,
    "source_hash": "b4efff54",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pip 22.2.2 from /opt/anaconda3/lib/python3.8/site-packages/pip (python 3.8)\n",
      "Collecting tweepy==4.10\n",
      "  Using cached tweepy-4.10.0-py3-none-any.whl (94 kB)\n",
      "Collecting oauthlib<4,>=3.2.0\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Collecting requests<3,>=2.27.0\n",
      "  Using cached requests-2.28.2-py3-none-any.whl (62 kB)\n",
      "Collecting requests-oauthlib<2,>=1.2.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.0.1-cp38-cp38-macosx_10_9_x86_64.whl (122 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
      "Installing collected packages: charset-normalizer, urllib3, oauthlib, idna, certifi, requests, requests-oauthlib, tweepy\n",
      "  changing mode of /opt/anaconda3/bin/normalizer to 755\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.20 requires pathlib, which is not installed.\n",
      "anaconda-project 0.11.1 requires ruamel-yaml, which is not installed.\n",
      "distributed 2022.7.1 requires tornado<6.2,>=6.0.3, but you have tornado 6.2 which is incompatible.\n",
      "conda-repo-cli 1.0.20 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "conda-repo-cli 1.0.20 requires nbformat==5.4.0, but you have nbformat 5.5.0 which is incompatible.\n",
      "conda-repo-cli 1.0.20 requires requests==2.28.1, but you have requests 2.28.2 which is incompatible.\n",
      "aiohttp 3.8.1 requires charset-normalizer<3.0,>=2.0, but you have charset-normalizer 3.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed certifi-2022.12.7 charset-normalizer-3.0.1 idna-3.4 oauthlib-3.2.2 requests-2.28.2 requests-oauthlib-1.3.1 tweepy-4.10.0 urllib3-1.26.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -Iv tweepy==4.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "9e0dc46ac21d4f11b3f8b11e323014ee",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 8,
    "execution_start": 1673845655332,
    "source_hash": "754bfc36",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "ca888baeb4eb46bf8aa5ca0e3dc7f893",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1673858292151,
    "source_hash": "7a65f825"
   },
   "outputs": [],
   "source": [
    "# for the twitter section\n",
    "import tweepy\n",
    "import os\n",
    "import datetime\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "# for the lyrics scrape section\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict, Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "f4b294822e9845609aef3686c627b3a7",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1673858425223,
    "source_hash": "e2837df1"
   },
   "outputs": [],
   "source": [
    "# Use this cell for any import statements you add\n",
    "import pandas as pd\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d45d582257ae4c6ba7a28ae54b911154",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "We need bring in our API keys. Since API keys should be kept secret, we'll keep them in a file called `api_keys.py`. This file should be stored in the directory where you store this notebook. The example file is provided for you on Blackboard. The example has API keys that are _not_ functional, so you'll need to get Twitter credentials and replace the placeholder keys. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "5640fc5c266649ff9f432ddb0a34db09",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 40,
    "execution_start": 1673845655355,
    "source_hash": "9d1035f5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# API Info From Twiiter\n",
    "api_key = 'lUbawTeSALmauWiZRDtiO1ODC'\n",
    "api_key_secret = 'p1tEzvVuq9gbhXKUPssvDXW5LsIzOY48act4nx6HhdkUrMz7RE'\n",
    "bearer_token = 'AAAAAAAAAAAAAAAAAAAAAPGplAEAAAAAau1b3QCVGUGw6XqeH6jiKi48qUM%3DNy100XgODOQXs2T1d00l42toQa0v3Zbn0nPt5QYu1yM5pFH2od'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "fdfb41db2c474e0a864c7e00b716b16d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1673845655395,
    "source_hash": "a3b859a9"
   },
   "outputs": [],
   "source": [
    "#from api_keys import api_key, api_key_secret, bearer_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "47ed73b88c3b4aab90f8b69ad024f316",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1673845655396,
    "source_hash": "bbfcb2a8"
   },
   "outputs": [],
   "source": [
    "client = tweepy.Client(bearer_token,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "35e43eb8844b4cacb49e3a0b9ec73aec",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Testing the API\n",
    "\n",
    "The Twitter APIs are quite rich. Let's play around with some of the features before we dive into this section of the assignment. For our testing, it's convenient to have a small data set to play with. We will seed the code with the handle of John Chandler, one of the instructors in this course. His handle is `@37chandler`. Feel free to use a different handle if you would like to look at someone else's data. \n",
    "\n",
    "We will write code to explore a few aspects of the API: \n",
    "\n",
    "1. Pull some of the followers @37chandler.\n",
    "1. Explore response data, which gives us information about Twitter users. \n",
    "1. Pull the last few tweets by @37chandler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "cell_id": "89313fcd4fdd40318f06365f7dca8aca",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 273,
    "execution_start": 1673845655396,
    "source_hash": "362d8c08"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 693 seconds.\n"
     ]
    }
   ],
   "source": [
    "handle = \"37chandler\"\n",
    "\n",
    "user_obj = client.get_user(username=handle)\n",
    "\n",
    "followers = client.get_users_followers(\n",
    "    # Learn about user fields here: \n",
    "    # https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/user\n",
    "    user_obj.data.id, user_fields=[\"created_at\",\"description\",\"location\",\n",
    "                                   \"public_metrics\"]\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f852db4fb6b144b8b8998d74b70846b4",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Now let's explore these a bit. We'll start by printing out names, locations, following count, and followers count for these users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "cell_id": "831d9e40c7914188bf079313f35d8551",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 7,
    "execution_start": 1673845655671,
    "source_hash": "d24a433f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John chandler lists 'Decatur, GA' as their location.\n",
      " Following: 129, Followers: 10.\n",
      "\n",
      "Frank P Seidl lists 'Twin Cities, Minnesota USA' as their location.\n",
      " Following: 37754, Followers: 37554.\n",
      "\n",
      "Roberta lists 'Salinas' as their location.\n",
      " Following: 1883, Followers: 181.\n",
      "\n",
      "Anna bikes MKE lists 'mke ' as their location.\n",
      " Following: 2287, Followers: 1757.\n",
      "\n",
      "Catherine lists 'San Angelo' as their location.\n",
      " Following: 2176, Followers: 223.\n",
      "\n",
      "Lisa lists 'None' as their location.\n",
      " Following: 2596, Followers: 831.\n",
      "\n",
      "Lexi lists 'None' as their location.\n",
      " Following: 432, Followers: 25.\n",
      "\n",
      "Dave Renn lists 'None' as their location.\n",
      " Following: 98, Followers: 10.\n",
      "\n",
      "Lionel lists 'None' as their location.\n",
      " Following: 200, Followers: 200.\n",
      "\n",
      "Megan Randall lists 'None' as their location.\n",
      " Following: 142, Followers: 98.\n",
      "\n",
      "Jacob Salzman lists 'None' as their location.\n",
      " Following: 563, Followers: 136.\n",
      "\n",
      "twiter not fun lists 'None' as their location.\n",
      " Following: 218, Followers: 20.\n",
      "\n",
      "Christian Tinsley lists 'None' as their location.\n",
      " Following: 11, Followers: 0.\n",
      "\n",
      "Steve lists 'I'm over here.' as their location.\n",
      " Following: 1626, Followers: 31.\n",
      "\n",
      "John O'Connor 🇺🇦 lists 'None' as their location.\n",
      " Following: 13, Followers: 1.\n",
      "\n",
      "CodeGrade lists 'Amsterdam' as their location.\n",
      " Following: 2772, Followers: 439.\n",
      "\n",
      "Cleverhood lists 'Providence, RI' as their location.\n",
      " Following: 2760, Followers: 3634.\n",
      "\n",
      "Regina 🚶‍♀️🚲🌳 lists 'Minneapolis' as their location.\n",
      " Following: 2829, Followers: 3131.\n",
      "\n",
      "Eric Hallstrom lists 'Missoula, MT' as their location.\n",
      " Following: 474, Followers: 292.\n",
      "\n",
      "Tyler 📊 🐕 🚲 lists 'Minneapolis, MN' as their location.\n",
      " Following: 586, Followers: 110.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_to_print = 20\n",
    "\n",
    "for idx, user in enumerate(followers.data) :\n",
    "    following_count = user.public_metrics['following_count']\n",
    "    followers_count = user.public_metrics['followers_count']\n",
    "    \n",
    "    print(f\"{user.name} lists '{user.location}' as their location.\")\n",
    "    print(f\" Following: {following_count}, Followers: {followers_count}.\")\n",
    "    print()\n",
    "    \n",
    "    if idx >= (num_to_print - 1) :\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7ce339f5cfe942b5aa46084630c6f1c8",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Let's find the person who follows this handle who has the most followers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "cell_id": "a837de5ab6e7465e939d0e863fdb109d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3,
    "execution_start": 1673845655719,
    "source_hash": "89bc9eaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaceConscious\n",
      "{'followers_count': 37554, 'following_count': 37754, 'tweet_count': 13956, 'listed_count': 305}\n"
     ]
    }
   ],
   "source": [
    "max_followers = 0\n",
    "\n",
    "for idx, user in enumerate(followers.data) :\n",
    "    followers_count = user.public_metrics['followers_count']\n",
    "    \n",
    "    if followers_count > max_followers :\n",
    "        max_followers = followers_count\n",
    "        max_follower_user = user\n",
    "\n",
    "        \n",
    "print(max_follower_user)\n",
    "print(max_follower_user.public_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e9e7767867a945159c3f405c7a28cd1f",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Let's pull some more user fields and take a look at them. The fields can be specified in the `user_fields` argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "cell_id": "aef9699479c6452789597cba83a782ab",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 46,
    "execution_start": 1673845655721,
    "source_hash": "562f0180"
   },
   "outputs": [],
   "source": [
    "response = client.get_user(id=user_obj.data.id,\n",
    "                          user_fields=[\"created_at\",\"description\",\"location\",\n",
    "                                       \"entities\",\"name\",\"pinned_tweet_id\",\"profile_image_url\",\n",
    "                                       \"verified\",\"public_metrics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "cell_id": "1005f496e31541df8b89118d241766a9",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 13,
    "execution_start": 1673845655768,
    "source_hash": "7119728e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for description we have He/Him. Data scientist, urban cyclist, educator, erstwhile frisbee player. \n",
      "\n",
      "¯\\_(ツ)_/¯\n",
      "for profile_image_url we have https://pbs.twimg.com/profile_images/2680483898/b30ae76f909352dbae5e371fb1c27454_normal.png\n",
      "for public_metrics we have {'followers_count': 185, 'following_count': 592, 'tweet_count': 1049, 'listed_count': 3}\n",
      "for verified we have False\n",
      "for created_at we have 2009-04-18 22:08:22+00:00\n",
      "for location we have MN\n",
      "for name we have John Chandler\n",
      "for username we have 37chandler\n",
      "for id we have 33029025\n"
     ]
    }
   ],
   "source": [
    "for field, value in response.data.items() :\n",
    "    print(f\"for {field} we have {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9273ed595d774269865ec3efbbbd07a7",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Now a few questions for you about the user object.\n",
    "\n",
    "Q: How many fields are being returned in the `response` object? \n",
    "\n",
    "A: 8 fields. These fields are as follows:  \"created_at\",\"description\",\"location\", \"entities\",\"name\",\"pinned_tweet_id\",\"profile_image_url\", \"verified\",\"public_metrics\"\n",
    "\n",
    "---\n",
    "\n",
    "Q: Are any of the fields within the user object non-scalar? (I.e., more complicated than a simple data type like integer, float, string, boolean, etc.) \n",
    "\n",
    "A: Yes the public metrics field and created_at field are non-scalar data types. Non-scalar data types include arrays, objects, maps, set, and graphs. The public metrics includes a field that can be considered as an array or set where there is a collection of items stored. In addition, the created_at field is a datetime format represented as an object rather than a scalar data type.\n",
    "\n",
    "---\n",
    "\n",
    "Q: How many friends, followers, and tweets does this user have? \n",
    "\n",
    "A: \n",
    "Friends - 183\n",
    "Followers - 591\n",
    "Tweets - 1049\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "fbb11babf55f459e810d66dc2dcb28e1",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Although you won't need it for this assignment, individual tweets can be a rich source of text-based data. To illustrate the concepts, let's look at the last few tweets for this user. You are encouraged to explore the fields that are available about Tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "cell_id": "a613754323d946cb9414008f0bb7be92",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 219,
    "execution_start": 1673845655770,
    "source_hash": "462e3762"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1611545485029810180\n",
      "Happy Dia de los Reyes to all who celebrate it. https://t.co/4G7zAuwC70\n",
      "\n",
      "1608230093071212544\n",
      "RT @year_progress: ▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓ 99%\n",
      "\n",
      "1606038920604499969\n",
      "RT @CoachBalto: This video is perfect. Parents please watch. Part 1/2 https://t.co/NvcBFmyFPO\n",
      "\n",
      "1602407567036190743\n",
      "RT @LindsayMasland: I had the realization that \"grades are pretend\" the first time I taught (as a TA).  \n",
      "\n",
      "I was grading something with both…\n",
      "\n",
      "1598645130075856896\n",
      "RT @marinaendicott: My new favourite lawyer’s letter, just for the sheer joy of the tone.\n",
      "\n",
      "1598156055997222912\n",
      "RT @_TanHo: Hey friends, #AdventOfCode starts TONIGHT! I've organized a friendly leaderboard every year for the #rstats (and friends) commu…\n",
      "\n",
      "1597746144108740608\n",
      "If you like biking and not getting hit by muederboxes, you should consider one of these. https://t.co/0prMLbvj3b\n",
      "\n",
      "1597734124927995904\n",
      "RT @CraigTheDev: A lot of people argue that AI art isn't theft as it isn't copying the original images but referencing them like a person.…\n",
      "\n",
      "1597641859509415936\n",
      "RT @nytimesbooks: Here’s our full list of the 10 Best Books of 2022. Learn more about each title here. https://t.co/DtsSSlHyJg https://t.co…\n",
      "\n",
      "1597628397391208448\n",
      "@KevinQ @UpshotNYT Or maybe the effect is the same for every team, since everyone has some unlikely edge probabilities filling out their graph.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = client.get_users_tweets(user_obj.data.id)\n",
    "\n",
    "# By default, only the ID and text fields of each Tweet will be returned\n",
    "for idx, tweet in enumerate(response.data) :\n",
    "    print(tweet.id)\n",
    "    print(tweet.text)\n",
    "    print()\n",
    "    \n",
    "    if idx > 10 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "3baac78a8eb6428da05acb365244f964",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Pulling Follower Information\n",
    "\n",
    "In this next section of the assignment, we will pull information about the followers of your two artists. We've seen above how to pull a set of followers using `client.get_users_followers`. This function has a parameter, `max_results`, that we can use to change the number of followers that we pull. Unfortunately, we can only pull 1000 followers at a time, which means we will need to handle the _pagination_ of our results. \n",
    "\n",
    "The return object has the `.data` field, where the results will be found. It also has `.meta`, which we use to select the next \"page\" in the results using the `next_token` result. I will illustrate the ideas using our user from above. \n",
    "\n",
    "\n",
    "### Rate Limiting\n",
    "\n",
    "Twitter limits the rates at which we can pull data, as detailed in [this guide](https://developer.twitter.com/en/docs/twitter-api/rate-limits). We can make 15 user requests per 15 minutes, meaning that we can pull $4 \\cdot 15 \\cdot 1000 = 60000$ users per hour. I illustrate the handling of rate limiting below, though whether or not you hit that part of the code depends on your value of `handle`.  \n",
    "\n",
    "\n",
    "In the below example, I'll pull all the followers, 25 at a time. (We're using 25 to illustrate the idea; when you do this set the value to 1000.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "cell_id": "a5f78cdd47994d58b2c850c5a8afe5ef",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 292,
    "execution_start": 1673845656038,
    "source_hash": "71ebcd1d"
   },
   "outputs": [],
   "source": [
    "handle_followers = []\n",
    "pulls = 0\n",
    "max_pulls = 100\n",
    "next_token = None\n",
    "\n",
    "while True :\n",
    "\n",
    "    followers = client.get_users_followers(\n",
    "        user_obj.data.id, \n",
    "        max_results=1000, # when you do this for real, set this to 1000!\n",
    "        pagination_token = next_token,\n",
    "        user_fields=[\"created_at\",\"description\",\"location\",\n",
    "                     \"entities\",\"name\",\"pinned_tweet_id\",\"profile_image_url\",\n",
    "                     \"verified\",\"public_metrics\"]\n",
    "    )\n",
    "    pulls += 1\n",
    "    \n",
    "    for follower in followers.data : \n",
    "        follower_row = (follower.id,follower.name,follower.created_at,follower.description)\n",
    "        handle_followers.append(follower_row)\n",
    "    \n",
    "    if 'next_token' in followers.meta and pulls < max_pulls :\n",
    "        next_token = followers.meta['next_token']\n",
    "    else : \n",
    "        break\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "cell_id": "d3e3d26eb47d47b387603c2aebb1e7af",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1673850616951,
    "source_hash": "ee35ede7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "handle_followers = []\n",
    "pulls = 0\n",
    "max_pulls = 200000\n",
    "next_token = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "fd4d8ff825c94ca69db87eae3ca78c61",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Pulling Twitter Data for Your Artists\n",
    "\n",
    "Now let's take a look at your artists and see how long it is going to take to pull all their followers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "882eede86c4e49b1bd637c607118553b",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 107,
    "execution_start": 1673845656331,
    "source_hash": "e2e061d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It would take 5.44 hours to pull all 326298 followers for billyjoel. \n",
      "It would take 3.68 hours to pull all 221066 followers for omarapollo. \n"
     ]
    }
   ],
   "source": [
    "artists = dict()\n",
    "\n",
    "for handle in ['billyjoel','omarapollo'] : \n",
    "    user_obj = client.get_user(username=handle,user_fields=[\"public_metrics\"])\n",
    "    artists[handle] = (user_obj.data.id, \n",
    "                       handle,\n",
    "                       user_obj.data.public_metrics['followers_count'])\n",
    "    \n",
    "\n",
    "for artist, data in artists.items() : \n",
    "    print(f\"It would take {data[2]/(1000*15*4):.2f} hours to pull all {data[2]} followers for {artist}. \")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "2c4129c15bd840779237ab173ffc2e4a",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Depending on what you see in the display above, you may want to limit how many followers you pull. It'd be great to get at least 200,000 per artist. \n",
    "\n",
    "As we pull data for each artist we will write their data to a folder called \"twitter\", so we will make that folder if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "49ec19f8478d47d89126fdaed041c83e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6,
    "execution_start": 1673845656440,
    "source_hash": "908a3494"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twitter removed.\n",
      "twitter created.\n"
     ]
    }
   ],
   "source": [
    "# Make the \"twitter\" folder here. \n",
    "folder_name = \"twitter\"\n",
    "\n",
    "#If you'd like to practice your programming, add functionality \n",
    "# that checks to see if the folder exists. If it does, then \"unlink\" it. Then create a new one.\n",
    "\n",
    "if os.path.exists(folder_name):\n",
    "    os.rmdir(folder_name)\n",
    "    print(f\"{folder_name} removed.\")\n",
    "\n",
    "os.mkdir(folder_name)\n",
    "print(f\"{folder_name} created.\")\n",
    "\n",
    "if not os.path.isdir(\"twitter\") : \n",
    "    #shutil.rmtree(\"twitter/\")\n",
    "    os.mkdir(\"twitter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6aa57088e83a4cea830d3c38f69688e7",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "In this following cells, build on the above code to pull some of the followers and their data for your two artists. As you pull the data, write the follower ids to a file called `[artist name]_followers.txt` in the \"twitter\" folder. For instance, for Cher I would create a file named `cher_followers.txt`. As you pull the data, also store it in an object like a list or a data frame.\n",
    "\n",
    "In addition to creating a file that only has follower IDs in it, you will create a file that includes user data. From the response object please extract and store the following fields: \n",
    "\n",
    "* screen_name\t\n",
    "* name\t\n",
    "* id\t\n",
    "* location\t\n",
    "* followers_count\t\n",
    "* friends_count\t\n",
    "* description\n",
    "\n",
    "Store the fields with one user per row in a tab-delimited text file with the name `[artist name]_follower_data.txt`. For instance, for Cher I would create a file named `cher_follower_data.txt`. \n",
    "\n",
    "One note: the user's description can have tabs or returns in it, so make sure to clean those out of the description before writing them to the file. I've included some example code to do that below the stub. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "53435a18e76a4d33a450a296427505f6",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3,
    "execution_start": 1673845656451,
    "source_hash": "aad365eb"
   },
   "outputs": [],
   "source": [
    "num_followers_to_pull = 200*1000 # feel free to use this to limit the number of followers you pull."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "allow_embed": false,
    "cell_id": "57278e4ea2064a9e98f64b385b93a4eb",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1906680,
    "execution_start": 1673856357760,
    "source_hash": "83e7e7f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:03.787007\n"
     ]
    }
   ],
   "source": [
    "# Modify the below code stub to pull the follower IDs and write them to a file.\n",
    "pulls = 0\n",
    "max_pulls = 4\n",
    "next_token = None\n",
    "follower_Data =[]\n",
    "handles = [\"billyjoel\", \"omarapollo\"]\n",
    "\n",
    "whitespace_pattern = re.compile(r\"\\s+\")\n",
    "\n",
    "user_data = dict()\n",
    "followers_data = dict()\n",
    "\n",
    "\n",
    "for handle in handles:\n",
    "    user_data[handle] = []  # will be a list of lists\n",
    "    followers_data[handle] = []  # will be a simple list of IDs\n",
    "\n",
    "\n",
    "# Grabs the time when we start making requests to the API\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "for handle in handles:\n",
    "\n",
    "    # Create the output file names\n",
    "\n",
    "    followers_output_file = handle + \"_follower1.txt\"\n",
    "    user_data_output_file = handle + \"_follower_data1.txt\"\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # Pull User Data\n",
    "        user_obj = client.get_user(\n",
    "            username=handle)\n",
    "        # Pull Follower ID\n",
    "        #followers = client.get_users_followers(user_obj.data.id, user_fields=[\"id\"])\n",
    "        followers_id = {\"id\": []}\n",
    "   \n",
    "    # Pull Follower Data\n",
    "        Followers = client.get_users_followers(\n",
    "            user_obj.data.id,\n",
    "            max_results=1000,\n",
    "            pagination_token=next_token,\n",
    "            user_fields=[\n",
    "                \"username\",\n",
    "                \"description\",\n",
    "                \"name\",\n",
    "                \"id\",\n",
    "                'location',\n",
    "                \"public_metrics\",\n",
    "         ],)\n",
    "        pulls += 1\n",
    "        \n",
    "        #for follower in Follwers.data:\n",
    "            #follor_row = (Follower.id, Follower.screen_nam, Follower.username, Follower.location, Follower.public_metrics, Follower.description)\n",
    "            \n",
    "        \n",
    "\n",
    "        if 'next_token' in Followers.meta and pulls < max_pulls :\n",
    "            next_token = Followers.meta['next_token']\n",
    "        else : \n",
    "            break\n",
    "        # For each response object, extract the needed fields and store them in a dictionary or # data frame.\n",
    "        Twitter_Data = {\n",
    "            \"screen_name\": [],\n",
    "            \"name\": [],\n",
    "            \"id\": [],\n",
    "            \"location\": [],\n",
    "            \"followers_count\": [],\n",
    "            \"friends_count\": [],\n",
    "            \"description\": [], }\n",
    "        \n",
    "    for idx, user in enumerate(Followers.data):\n",
    "        Twitter_Data[\"screen_name\"].append(user.username),\n",
    "        Twitter_Data[\"name\"].append(user.name),\n",
    "        followers_id[\"id\"].append(user.id)\n",
    "        Twitter_Data[\"id\"].append(user.id),\n",
    "        Twitter_Data[\"location\"].append(user.location),\n",
    "        Twitter_Data[\"description\"].append(user.description),\n",
    "        followers_count = user.public_metrics[\"followers_count\"]\n",
    "        Twitter_Data[\"followers_count\"].append(followers_count),\n",
    "        following_count = user.public_metrics[\"following_count\"]\n",
    "        Twitter_Data[\"friends_count\"].append(following_count),\n",
    "     # Create Data Frames\n",
    "    followers_df = pd.DataFrame(followers_id)\n",
    "    followers_data_df = pd.DataFrame(Twitter_Data)\n",
    "\n",
    "    # Clean Description\n",
    "    followers_data_df['description'] = followers_data_df['description'].apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
    "\n",
    "    # Create a path to twitter folder and check if folder does not exist, create one\n",
    "    folder_path = os.path.join(os.getcwd(), folder_name)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    # Direct txt file to twitter folder\n",
    "    followers_output_file_path = os.path.join(folder_path, followers_output_file)\n",
    "\n",
    "    # Write over txt file\n",
    "    with open(followers_output_file, \"w\") as output_file1:\n",
    "        output_file1.write(followers_df.to_string())\n",
    "    # Direct txt file to twitter folder\n",
    "    user_data_output_file_path = os.path.join(folder_path, user_data_output_file)\n",
    "\n",
    "    # Write over txt file\n",
    "    with open(user_data_output_file, \"w\") as output_file2:\n",
    "        output_file2.write(followers_data_df.to_string())\n",
    "\n",
    "\n",
    "# Let's see how long it took to grab all follower IDs\n",
    "end_time = datetime.datetime.now()\n",
    "print(end_time - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "cell_id": "9df016ae5a854ab59783b0e53e4351f2",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1673846659727,
    "source_hash": "fb6722f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Home by Warsan Shire no one leaves home unless home is the mouth of a shark. you only run for the border when you see the whole city running as well.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tricky_description = \"\"\"\n",
    "    Home by Warsan Shire\n",
    "    \n",
    "    no one leaves home unless\n",
    "    home is the mouth of a shark.\n",
    "    you only run for the border\n",
    "    when you see the whole city\n",
    "    running as well.\n",
    "\n",
    "\"\"\"\n",
    "# This won't work in a tab-delimited text file.\n",
    "\n",
    "clean_description = re.sub(r\"\\s+\",\" \",tricky_description).strip()\n",
    "clean_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "94267e258ee24738b51887af1fa35db4",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "# Lyrics Scrape\n",
    "\n",
    "This section asks you to pull data from the Twitter API and scrape www.AZLyrics.com. In the notebooks where you do that work you are asked to store the data in specific ways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "cell_id": "da6a3d44d87c4ab3892cc0a250ea0bc9",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1673858274642,
    "source_hash": "5241f4e4"
   },
   "outputs": [],
   "source": [
    "artists = {'Omar Apollo':\"https://www.azlyrics.com/o/omarapollo.html\",\n",
    "           'billyjoel':\"https://www.azlyrics.com/b/billyjoel.html\"} \n",
    "# we'll use this dictionary to hold both the artist name and the link on AZlyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "38951e2c25954010a63fa4c962ac2fea",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## A Note on Rate Limiting\n",
    "\n",
    "The lyrics site, www.azlyrics.com, does not have an explicit maximum on number of requests in any one time, but in our testing it appears that too many requests in too short a time will cause the site to stop returning lyrics pages. (Entertainingly, the page that gets returned seems to only have the song title to [a Tom Jones song](https://www.azlyrics.com/lyrics/tomjones/itsnotunusual.html).) \n",
    "\n",
    "Whenever you call `requests.get` to retrieve a page, put a `time.sleep(5 + 10*random.random())` on the next line. This will help you not to get blocked. If you _do_ get blocked, which you can identify if the returned pages are not correct, just request a lyrics page through your browser. You'll be asked to perform a CAPTCHA and then your requests should start working again. \n",
    "\n",
    "## Part 1: Finding Links to Songs Lyrics\n",
    "\n",
    "That general artist page has a list of all songs for that artist with links to the individual song pages. \n",
    "\n",
    "Q: Take a look at the `robots.txt` page on www.azlyrics.com. (You can read more about these pages [here](https://developers.google.com/search/docs/advanced/robots/intro).) Is the scraping we are about to do allowed or disallowed by this page? How do you know? \n",
    "\n",
    "A: This robots.txt file specifies which pages or sections of the website can be accessed by web crawlers or agents. The \"User-agent\" field determines which rules apply to which web crawler or agent. A \"*\" means that the rules apply to all web crawlers or agents.\n",
    "\n",
    "The \"Disallow\" rules (\"Disallow: /lyricsdb/\" and \"Disallow: /song/\") prohibit web crawlers or agents from accessing the \"lyricsdb\" and \"song\" directories respectively. The \"Allow\" rule (\"Allow: /\") allows all other pages on the website to be accessed by web crawlers or agents.\n",
    "\n",
    "For a specific user-agent \"008\", the \"Disallow\" rule (\"Disallow: /\") prohibits access to all pages on the website.\n",
    "\n",
    "In summary, this robots.txt file prohibits web scraping for the user-agent \"008\" on any pages of the website, and for other web crawlers and agents, web scraping is allowed on all pages except the \"lyricsdb\" and \"song\" directories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "cell_id": "d251995f0b2c4784b2ccbc2966de2114",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 14438,
    "execution_start": 1673858960756,
    "source_hash": "2a3b00a2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "artists = {'Omar Apollo':\"https://www.azlyrics.com/o/omarapollo.html\",\n",
    "           'billyjoel':\"https://www.azlyrics.com/b/billyjoel.html\"} \n",
    "# Let's set up a dictionary of lists to hold our links\n",
    "lyrics_pages = defaultdict(list)\n",
    "\n",
    "for artist, artist_page in artists.items() :\n",
    "    # request the page and sleep\n",
    "    r = requests.get(artist_page)\n",
    "    time.sleep(5 + 10*random.random())\n",
    "\n",
    "    # now extract the links to lyrics pages from this page\n",
    "    BSoup = BeautifulSoup(r.content, 'html.parser')\n",
    "    links = BSoup.find_all('a',href=lambda x: x and x.startswith('/lyrics'))\n",
    "    links = [link.get('href') for link in links]\n",
    "\n",
    "\n",
    "    \n",
    "    # store the links `lyrics_pages` where the key is the artist and the\n",
    "    \n",
    "\n",
    "\n",
    "    # value is a list of links. \n",
    "    lyrics_pages[artist] = links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "cell_id": "43823a5490c54c7cb0ecd3b68c07088c",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5,
    "execution_start": 1673859426523,
    "source_hash": "c9b7922",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'Omar Apollo': ['/lyrics/omarapollo/stereointro.html',\n",
       "              '/lyrics/omarapollo/ignorin.html',\n",
       "              '/lyrics/omarapollo/erase.html',\n",
       "              '/lyrics/omarapollo/ugotme.html',\n",
       "              '/lyrics/omarapollo/hijodesumadre.html',\n",
       "              '/lyrics/omarapollo/lucky.html',\n",
       "              '/lyrics/omarapollo/amormalo.html',\n",
       "              '/lyrics/omarapollo/ashamed.html',\n",
       "              '/lyrics/omarapollo/kickback.html',\n",
       "              '/lyrics/omarapollo/friends.html',\n",
       "              '/lyrics/omarapollo/thereformeinterlude.html',\n",
       "              '/lyrics/omarapollo/hearingyourvoice.html',\n",
       "              '/lyrics/omarapollo/sogood.html',\n",
       "              '/lyrics/omarapollo/trouble.html',\n",
       "              '/lyrics/omarapollo/imamazing.html',\n",
       "              '/lyrics/omarapollo/kamikaze.html',\n",
       "              '/lyrics/omarapollo/wantuaround.html',\n",
       "              '/lyrics/omarapollo/stayback.html',\n",
       "              '/lyrics/omarapollo/heyboy.html',\n",
       "              '/lyrics/omarapollo/dosunonueve219.html',\n",
       "              '/lyrics/omarapollo/useless.html',\n",
       "              '/lyrics/omarapollo/bifren.html',\n",
       "              '/lyrics/omarapollo/thetwoofus.html',\n",
       "              '/lyrics/omarapollo/ivory.html',\n",
       "              '/lyrics/omarapollo/talk.html',\n",
       "              '/lyrics/omarapollo/nogoodreason.html',\n",
       "              '/lyrics/omarapollo/invincible.html',\n",
       "              '/lyrics/omarapollo/endlesslyinterlude.html',\n",
       "              '/lyrics/omarapollo/killingme.html',\n",
       "              '/lyrics/omarapollo/goaway.html',\n",
       "              '/lyrics/omarapollo/waitingonyou.html',\n",
       "              '/lyrics/omarapollo/petrified.html',\n",
       "              '/lyrics/omarapollo/personally.html',\n",
       "              '/lyrics/omarapollo/enelolvido.html',\n",
       "              '/lyrics/omarapollo/tamagotchi.html',\n",
       "              '/lyrics/omarapollo/cantgetoveryou.html',\n",
       "              '/lyrics/omarapollo/evergreen.html',\n",
       "              '/lyrics/omarapollo/badlife.html',\n",
       "              '/lyrics/omarapollo/mrneighbor.html',\n",
       "              '/lyrics/omarapollo/endlessly.html',\n",
       "              '/lyrics/omarapollo/highlight.html',\n",
       "              '/lyrics/omarapollo/archetype.html',\n",
       "              '/lyrics/omarapollo/savingallmylove.html',\n",
       "              '/lyrics/omarapollo/prettyboy.html',\n",
       "              '/lyrics/omarapollo/algo.html',\n",
       "              '/lyrics/omarapollo/beautyboy.html',\n",
       "              '/lyrics/omarapollo/brakelights.html',\n",
       "              '/lyrics/omarapollo/californiadreamin.html',\n",
       "              '/lyrics/omarapollo/frio.html',\n",
       "              '/lyrics/omarapollo/hitmeup.html',\n",
       "              '/lyrics/omarapollo/imagineu.html',\n",
       "              '/lyrics/omarapollo/kissyew.html',\n",
       "              '/lyrics/omarapollo/pram.html',\n",
       "              '/lyrics/omarapollo/staybackremix.html',\n",
       "              '/lyrics/omarapollo/today.html',\n",
       "              '/lyrics/omarapollo/unbothered.html'],\n",
       "             'billyjoel': ['/lyrics/billyjoel/shesgotaway.html',\n",
       "              '/lyrics/billyjoel/youcanmakemefree.html',\n",
       "              '/lyrics/billyjoel/everybodylovesyounow.html',\n",
       "              '/lyrics/billyjoel/whyjudywhy.html',\n",
       "              '/lyrics/billyjoel/fallingoftherain.html',\n",
       "              '/lyrics/billyjoel/turnaround.html',\n",
       "              '/lyrics/billyjoel/youlooksogoodtome.html',\n",
       "              '/lyrics/billyjoel/tomorrowistoday.html',\n",
       "              '/lyrics/billyjoel/gottobeginagain.html',\n",
       "              '/lyrics/billyjoel/travelinprayer.html',\n",
       "              '/lyrics/billyjoel/pianoman.html',\n",
       "              '/lyrics/billyjoel/aintnocrime.html',\n",
       "              '/lyrics/billyjoel/youremyhome.html',\n",
       "              '/lyrics/billyjoel/theballadofbillythekid.html',\n",
       "              '/lyrics/billyjoel/worsecomestoworst.html',\n",
       "              '/lyrics/billyjoel/stopinnevada.html',\n",
       "              '/lyrics/billyjoel/ifionlyhadthewordstotellyou.html',\n",
       "              '/lyrics/billyjoel/somewherealongtheline.html',\n",
       "              '/lyrics/billyjoel/captainjack.html',\n",
       "              '/lyrics/billyjoel/longlongtime.html',\n",
       "              '/lyrics/billyjoel/josephine.html',\n",
       "              '/lyrics/billyjoel/rosalinda.html',\n",
       "              '/lyrics/billyjoel/streetlifeserenader.html',\n",
       "              '/lyrics/billyjoel/losangelenos.html',\n",
       "              '/lyrics/billyjoel/thegreatsuburbanshowdown.html',\n",
       "              '/lyrics/billyjoel/roberta.html',\n",
       "              '/lyrics/billyjoel/theentertainer.html',\n",
       "              '/lyrics/billyjoel/lastofthebigtimespenders.html',\n",
       "              '/lyrics/billyjoel/weekendsong.html',\n",
       "              '/lyrics/billyjoel/souvenir.html',\n",
       "              '/lyrics/billyjoel/saygoodbyetohollywood.html',\n",
       "              '/lyrics/billyjoel/summerhighlandfalls.html',\n",
       "              '/lyrics/billyjoel/allyouwannadoisdance.html',\n",
       "              '/lyrics/billyjoel/newyorkstateofmind.html',\n",
       "              '/lyrics/billyjoel/james.html',\n",
       "              '/lyrics/billyjoel/preludeangryyoungman.html',\n",
       "              '/lyrics/billyjoel/ivelovedthesedays.html',\n",
       "              '/lyrics/billyjoel/miami2017seenthelightsgooutonbroadway.html',\n",
       "              '/lyrics/billyjoel/movinoutanthonyssong.html',\n",
       "              '/lyrics/billyjoel/thestranger.html',\n",
       "              '/lyrics/billyjoel/justthewayyouare.html',\n",
       "              '/lyrics/billyjoel/scenesfromanitalianrestaurant.html',\n",
       "              '/lyrics/billyjoel/vienna.html',\n",
       "              '/lyrics/billyjoel/onlythegooddieyoung.html',\n",
       "              '/lyrics/billyjoel/shesalwaysawoman.html',\n",
       "              '/lyrics/billyjoel/getitrightthefirsttime.html',\n",
       "              '/lyrics/billyjoel/everybodyhasadream.html',\n",
       "              '/lyrics/billyjoel/bigshot.html',\n",
       "              '/lyrics/billyjoel/honesty.html',\n",
       "              '/lyrics/billyjoel/mylife.html',\n",
       "              '/lyrics/billyjoel/zanzibar.html',\n",
       "              '/lyrics/billyjoel/stiletto.html',\n",
       "              '/lyrics/billyjoel/rosalindaseyes.html',\n",
       "              '/lyrics/billyjoel/halfamileaway.html',\n",
       "              '/lyrics/billyjoel/untilthenight.html',\n",
       "              '/lyrics/billyjoel/52ndstreet.html',\n",
       "              '/lyrics/billyjoel/youmayberight.html',\n",
       "              '/lyrics/billyjoel/sometimesafantasy.html',\n",
       "              '/lyrics/billyjoel/dontaskmewhy.html',\n",
       "              '/lyrics/billyjoel/itsstillrockandrolltome.html',\n",
       "              '/lyrics/billyjoel/allforleyna.html',\n",
       "              '/lyrics/billyjoel/idontwanttobealone.html',\n",
       "              '/lyrics/billyjoel/sleepingwiththetelevisionon.html',\n",
       "              '/lyrics/billyjoel/cetaittoiyouweretheone.html',\n",
       "              '/lyrics/billyjoel/closetotheborderline.html',\n",
       "              '/lyrics/billyjoel/throughthelongnight.html',\n",
       "              '/lyrics/billyjoel/allentown.html',\n",
       "              '/lyrics/billyjoel/laura.html',\n",
       "              '/lyrics/billyjoel/pressure.html',\n",
       "              '/lyrics/billyjoel/goodnightsaigon.html',\n",
       "              '/lyrics/billyjoel/shesrightontime.html',\n",
       "              '/lyrics/billyjoel/aroomofourown.html',\n",
       "              '/lyrics/billyjoel/surprises.html',\n",
       "              '/lyrics/billyjoel/scandinavianskies.html',\n",
       "              '/lyrics/billyjoel/wherestheorchestra.html',\n",
       "              '/lyrics/billyjoel/easymoney.html',\n",
       "              '/lyrics/billyjoel/aninnocentman.html',\n",
       "              '/lyrics/billyjoel/uptowngirl.html',\n",
       "              '/lyrics/billyjoel/thisnight.html',\n",
       "              '/lyrics/billyjoel/tellheraboutit.html',\n",
       "              '/lyrics/billyjoel/thelongesttime.html',\n",
       "              '/lyrics/billyjoel/carelesstalk.html',\n",
       "              '/lyrics/billyjoel/christielee.html',\n",
       "              '/lyrics/billyjoel/leaveatendermomentalone.html',\n",
       "              '/lyrics/billyjoel/keepingthefaith.html',\n",
       "              '/lyrics/billyjoel/runningonice.html',\n",
       "              '/lyrics/billyjoel/thisisthetime.html',\n",
       "              '/lyrics/billyjoel/amatteroftrust.html',\n",
       "              '/lyrics/billyjoel/modernwoman.html',\n",
       "              '/lyrics/billyjoel/babygrand.html',\n",
       "              '/lyrics/billyjoel/bigmanonmulberrystreet.html',\n",
       "              '/lyrics/billyjoel/temptation.html',\n",
       "              '/lyrics/billyjoel/codeofsilence.html',\n",
       "              '/lyrics/billyjoel/gettingcloser.html',\n",
       "              '/lyrics/billyjoel/thatsnotherstyle.html',\n",
       "              '/lyrics/billyjoel/wedidntstartthefire.html',\n",
       "              '/lyrics/billyjoel/thedowneasteralexa.html',\n",
       "              '/lyrics/billyjoel/igotoextremes.html',\n",
       "              '/lyrics/billyjoel/shameless.html',\n",
       "              '/lyrics/billyjoel/stormfront.html',\n",
       "              '/lyrics/billyjoel/leningrad.html',\n",
       "              '/lyrics/billyjoel/stateofgrace.html',\n",
       "              '/lyrics/billyjoel/wheninrome.html',\n",
       "              '/lyrics/billyjoel/andsoitgoes.html',\n",
       "              '/lyrics/billyjoel/nomansland.html',\n",
       "              '/lyrics/billyjoel/thegreatwallofchina.html',\n",
       "              '/lyrics/billyjoel/blondeoverblue.html',\n",
       "              '/lyrics/billyjoel/aminorvariation.html',\n",
       "              '/lyrics/billyjoel/shadesofgrey.html',\n",
       "              '/lyrics/billyjoel/allaboutsoul.html',\n",
       "              '/lyrics/billyjoel/lullabyegoodnightmyangel.html',\n",
       "              '/lyrics/billyjoel/theriverofdreams.html',\n",
       "              '/lyrics/billyjoel/twothousandyears.html',\n",
       "              '/lyrics/billyjoel/famouslastwords.html',\n",
       "              '/lyrics/billyjoel/aharddaysnight.html',\n",
       "              '/lyrics/billyjoel/allmylife.html',\n",
       "              '/lyrics/billyjoel/allshookup.html',\n",
       "              '/lyrics/billyjoel/auldlangsyne.html',\n",
       "              '/lyrics/billyjoel/backintheussr.html',\n",
       "              '/lyrics/billyjoel/crosstobear.html',\n",
       "              '/lyrics/billyjoel/dancetothemusic.html',\n",
       "              '/lyrics/billyjoel/dontworrybaby.html',\n",
       "              '/lyrics/billyjoel/elvispresleyblvd.html',\n",
       "              '/lyrics/billyjoel/heartbreakhotel.html',\n",
       "              '/lyrics/billyjoel/heygirl.html',\n",
       "              '/lyrics/billyjoel/highway61revisited.html',\n",
       "              '/lyrics/billyjoel/honkytonkwomen.html',\n",
       "              '/lyrics/billyjoel/houseofbluelight.html',\n",
       "              '/lyrics/billyjoel/illcryinstead.html',\n",
       "              '/lyrics/billyjoel/inasentimentalmood.html',\n",
       "              '/lyrics/billyjoel/lightasthebreeze.html',\n",
       "              '/lyrics/billyjoel/liveandletdie.html',\n",
       "              '/lyrics/billyjoel/maybeimamazed.html',\n",
       "              '/lyrics/billyjoel/moneyorlove.html',\n",
       "              '/lyrics/billyjoel/motorcyclesong.html',\n",
       "              '/lyrics/billyjoel/nobodyknowsbutme.html',\n",
       "              '/lyrics/billyjoel/onlyaman.html',\n",
       "              '/lyrics/billyjoel/oysterbay.html',\n",
       "              '/lyrics/billyjoel/theendoftheworld.html',\n",
       "              '/lyrics/billyjoel/thenightisstillyoung.html',\n",
       "              '/lyrics/billyjoel/thesiegfriedline.html',\n",
       "              '/lyrics/billyjoel/thetimestheyareachangin.html',\n",
       "              '/lyrics/billyjoel/theserhinestonedays.html',\n",
       "              '/lyrics/billyjoel/tomakeyoufeelmylove.html',\n",
       "              '/lyrics/billyjoel/whenyouwishuponastar.html',\n",
       "              '/lyrics/billyjoel/wherewereyouonourweddingday.html',\n",
       "              '/lyrics/billyjoel/whyshouldiworry.html',\n",
       "              '/lyrics/billyjoel/youpickedarealbadtime.html',\n",
       "              '/lyrics/billyjoel/youreonlyhumansecondwind.html']})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e112c70b68bc4b59b99ede9c235e1ad1",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Let's make sure we have enough lyrics pages to scrape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "cell_id": "80ff8ba6ba1f4f09bd1e4dcadecb1d5c",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "bbd00945"
   },
   "outputs": [],
   "source": [
    "for artist, lp in lyrics_pages.items() :\n",
    "    assert(len(set(lp)) > 20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "cell_id": "8300e88a83a64572a8ce1b837dee1ca4",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "76926a96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Omar Apollo we have 56.\n",
      "The full pull will take for this artist will take 0.16 hours.\n",
      "For billyjoel we have 149.\n",
      "The full pull will take for this artist will take 0.41 hours.\n"
     ]
    }
   ],
   "source": [
    "# Let's see how long it's going to take to pull these lyrics \n",
    "# if we're waiting `5 + 10*random.random()` seconds \n",
    "for artist, links in lyrics_pages.items() : \n",
    "    print(f\"For {artist} we have {len(links)}.\")\n",
    "    print(f\"The full pull will take for this artist will take {round(len(links)*10/3600,2)} hours.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "bc41590b5add4d2c96cc577fd1201fc7",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Part 2: Pulling Lyrics\n",
    "\n",
    "Now that we have the links to our lyrics pages, let's go scrape them! Here are the steps for this part. \n",
    "\n",
    "1. Create an empty folder in our repo called \"lyrics\". \n",
    "1. Iterate over the artists in `lyrics_pages`. \n",
    "1. Create a subfolder in lyrics with the artist's name. For instance, if the artist was Cher you'd have `lyrics/cher/` in your repo.\n",
    "1. Iterate over the pages. \n",
    "1. Request the page and extract the lyrics from the returned HTML file using BeautifulSoup.\n",
    "1. Use the function below, `generate_filename_from_url`, to create a filename based on the lyrics page, then write the lyrics to a text file with that name. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "cell_id": "7b48f3ab759c4ee8b0669f091c3eec77",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "66cf6682"
   },
   "outputs": [],
   "source": [
    "def generate_filename_from_link(link) :\n",
    "    \n",
    "    if not link :\n",
    "        return None\n",
    "    \n",
    "    # drop the http or https and the html\n",
    "    name = link.replace(\"https\",\"\").replace(\"http\",\"\")\n",
    "    name = link.replace(\".html\",\"\")\n",
    "\n",
    "    name = name.replace(\"/lyrics/\",\"\")\n",
    "    \n",
    "    # Replace useless chareacters with UNDERSCORE\n",
    "    name = name.replace(\"://\",\"\").replace(\".\",\"_\").replace(\"/\",\"_\")\n",
    "    \n",
    "    # tack on .txt\n",
    "    name = name + \".txt\"\n",
    "    \n",
    "    return(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/lyrics/billyjoel/shesgotaway.html',\n",
       " '/lyrics/billyjoel/youcanmakemefree.html',\n",
       " '/lyrics/billyjoel/everybodylovesyounow.html',\n",
       " '/lyrics/billyjoel/whyjudywhy.html',\n",
       " '/lyrics/billyjoel/fallingoftherain.html',\n",
       " '/lyrics/billyjoel/turnaround.html',\n",
       " '/lyrics/billyjoel/youlooksogoodtome.html',\n",
       " '/lyrics/billyjoel/tomorrowistoday.html',\n",
       " '/lyrics/billyjoel/gottobeginagain.html',\n",
       " '/lyrics/billyjoel/travelinprayer.html',\n",
       " '/lyrics/billyjoel/pianoman.html',\n",
       " '/lyrics/billyjoel/aintnocrime.html',\n",
       " '/lyrics/billyjoel/youremyhome.html',\n",
       " '/lyrics/billyjoel/theballadofbillythekid.html',\n",
       " '/lyrics/billyjoel/worsecomestoworst.html',\n",
       " '/lyrics/billyjoel/stopinnevada.html',\n",
       " '/lyrics/billyjoel/ifionlyhadthewordstotellyou.html',\n",
       " '/lyrics/billyjoel/somewherealongtheline.html',\n",
       " '/lyrics/billyjoel/captainjack.html',\n",
       " '/lyrics/billyjoel/longlongtime.html',\n",
       " '/lyrics/billyjoel/josephine.html',\n",
       " '/lyrics/billyjoel/rosalinda.html',\n",
       " '/lyrics/billyjoel/streetlifeserenader.html',\n",
       " '/lyrics/billyjoel/losangelenos.html',\n",
       " '/lyrics/billyjoel/thegreatsuburbanshowdown.html',\n",
       " '/lyrics/billyjoel/roberta.html',\n",
       " '/lyrics/billyjoel/theentertainer.html',\n",
       " '/lyrics/billyjoel/lastofthebigtimespenders.html',\n",
       " '/lyrics/billyjoel/weekendsong.html',\n",
       " '/lyrics/billyjoel/souvenir.html',\n",
       " '/lyrics/billyjoel/saygoodbyetohollywood.html',\n",
       " '/lyrics/billyjoel/summerhighlandfalls.html',\n",
       " '/lyrics/billyjoel/allyouwannadoisdance.html',\n",
       " '/lyrics/billyjoel/newyorkstateofmind.html',\n",
       " '/lyrics/billyjoel/james.html',\n",
       " '/lyrics/billyjoel/preludeangryyoungman.html',\n",
       " '/lyrics/billyjoel/ivelovedthesedays.html',\n",
       " '/lyrics/billyjoel/miami2017seenthelightsgooutonbroadway.html',\n",
       " '/lyrics/billyjoel/movinoutanthonyssong.html',\n",
       " '/lyrics/billyjoel/thestranger.html',\n",
       " '/lyrics/billyjoel/justthewayyouare.html',\n",
       " '/lyrics/billyjoel/scenesfromanitalianrestaurant.html',\n",
       " '/lyrics/billyjoel/vienna.html',\n",
       " '/lyrics/billyjoel/onlythegooddieyoung.html',\n",
       " '/lyrics/billyjoel/shesalwaysawoman.html',\n",
       " '/lyrics/billyjoel/getitrightthefirsttime.html',\n",
       " '/lyrics/billyjoel/everybodyhasadream.html',\n",
       " '/lyrics/billyjoel/bigshot.html',\n",
       " '/lyrics/billyjoel/honesty.html',\n",
       " '/lyrics/billyjoel/mylife.html',\n",
       " '/lyrics/billyjoel/zanzibar.html',\n",
       " '/lyrics/billyjoel/stiletto.html',\n",
       " '/lyrics/billyjoel/rosalindaseyes.html',\n",
       " '/lyrics/billyjoel/halfamileaway.html',\n",
       " '/lyrics/billyjoel/untilthenight.html',\n",
       " '/lyrics/billyjoel/52ndstreet.html',\n",
       " '/lyrics/billyjoel/youmayberight.html',\n",
       " '/lyrics/billyjoel/sometimesafantasy.html',\n",
       " '/lyrics/billyjoel/dontaskmewhy.html',\n",
       " '/lyrics/billyjoel/itsstillrockandrolltome.html',\n",
       " '/lyrics/billyjoel/allforleyna.html',\n",
       " '/lyrics/billyjoel/idontwanttobealone.html',\n",
       " '/lyrics/billyjoel/sleepingwiththetelevisionon.html',\n",
       " '/lyrics/billyjoel/cetaittoiyouweretheone.html',\n",
       " '/lyrics/billyjoel/closetotheborderline.html',\n",
       " '/lyrics/billyjoel/throughthelongnight.html',\n",
       " '/lyrics/billyjoel/allentown.html',\n",
       " '/lyrics/billyjoel/laura.html',\n",
       " '/lyrics/billyjoel/pressure.html',\n",
       " '/lyrics/billyjoel/goodnightsaigon.html',\n",
       " '/lyrics/billyjoel/shesrightontime.html',\n",
       " '/lyrics/billyjoel/aroomofourown.html',\n",
       " '/lyrics/billyjoel/surprises.html',\n",
       " '/lyrics/billyjoel/scandinavianskies.html',\n",
       " '/lyrics/billyjoel/wherestheorchestra.html',\n",
       " '/lyrics/billyjoel/easymoney.html',\n",
       " '/lyrics/billyjoel/aninnocentman.html',\n",
       " '/lyrics/billyjoel/uptowngirl.html',\n",
       " '/lyrics/billyjoel/thisnight.html',\n",
       " '/lyrics/billyjoel/tellheraboutit.html',\n",
       " '/lyrics/billyjoel/thelongesttime.html',\n",
       " '/lyrics/billyjoel/carelesstalk.html',\n",
       " '/lyrics/billyjoel/christielee.html',\n",
       " '/lyrics/billyjoel/leaveatendermomentalone.html',\n",
       " '/lyrics/billyjoel/keepingthefaith.html',\n",
       " '/lyrics/billyjoel/runningonice.html',\n",
       " '/lyrics/billyjoel/thisisthetime.html',\n",
       " '/lyrics/billyjoel/amatteroftrust.html',\n",
       " '/lyrics/billyjoel/modernwoman.html',\n",
       " '/lyrics/billyjoel/babygrand.html',\n",
       " '/lyrics/billyjoel/bigmanonmulberrystreet.html',\n",
       " '/lyrics/billyjoel/temptation.html',\n",
       " '/lyrics/billyjoel/codeofsilence.html',\n",
       " '/lyrics/billyjoel/gettingcloser.html',\n",
       " '/lyrics/billyjoel/thatsnotherstyle.html',\n",
       " '/lyrics/billyjoel/wedidntstartthefire.html',\n",
       " '/lyrics/billyjoel/thedowneasteralexa.html',\n",
       " '/lyrics/billyjoel/igotoextremes.html',\n",
       " '/lyrics/billyjoel/shameless.html',\n",
       " '/lyrics/billyjoel/stormfront.html',\n",
       " '/lyrics/billyjoel/leningrad.html',\n",
       " '/lyrics/billyjoel/stateofgrace.html',\n",
       " '/lyrics/billyjoel/wheninrome.html',\n",
       " '/lyrics/billyjoel/andsoitgoes.html',\n",
       " '/lyrics/billyjoel/nomansland.html',\n",
       " '/lyrics/billyjoel/thegreatwallofchina.html',\n",
       " '/lyrics/billyjoel/blondeoverblue.html',\n",
       " '/lyrics/billyjoel/aminorvariation.html',\n",
       " '/lyrics/billyjoel/shadesofgrey.html',\n",
       " '/lyrics/billyjoel/allaboutsoul.html',\n",
       " '/lyrics/billyjoel/lullabyegoodnightmyangel.html',\n",
       " '/lyrics/billyjoel/theriverofdreams.html',\n",
       " '/lyrics/billyjoel/twothousandyears.html',\n",
       " '/lyrics/billyjoel/famouslastwords.html',\n",
       " '/lyrics/billyjoel/aharddaysnight.html',\n",
       " '/lyrics/billyjoel/allmylife.html',\n",
       " '/lyrics/billyjoel/allshookup.html',\n",
       " '/lyrics/billyjoel/auldlangsyne.html',\n",
       " '/lyrics/billyjoel/backintheussr.html',\n",
       " '/lyrics/billyjoel/crosstobear.html',\n",
       " '/lyrics/billyjoel/dancetothemusic.html',\n",
       " '/lyrics/billyjoel/dontworrybaby.html',\n",
       " '/lyrics/billyjoel/elvispresleyblvd.html',\n",
       " '/lyrics/billyjoel/heartbreakhotel.html',\n",
       " '/lyrics/billyjoel/heygirl.html',\n",
       " '/lyrics/billyjoel/highway61revisited.html',\n",
       " '/lyrics/billyjoel/honkytonkwomen.html',\n",
       " '/lyrics/billyjoel/houseofbluelight.html',\n",
       " '/lyrics/billyjoel/illcryinstead.html',\n",
       " '/lyrics/billyjoel/inasentimentalmood.html',\n",
       " '/lyrics/billyjoel/lightasthebreeze.html',\n",
       " '/lyrics/billyjoel/liveandletdie.html',\n",
       " '/lyrics/billyjoel/maybeimamazed.html',\n",
       " '/lyrics/billyjoel/moneyorlove.html',\n",
       " '/lyrics/billyjoel/motorcyclesong.html',\n",
       " '/lyrics/billyjoel/nobodyknowsbutme.html',\n",
       " '/lyrics/billyjoel/onlyaman.html',\n",
       " '/lyrics/billyjoel/oysterbay.html',\n",
       " '/lyrics/billyjoel/theendoftheworld.html',\n",
       " '/lyrics/billyjoel/thenightisstillyoung.html',\n",
       " '/lyrics/billyjoel/thesiegfriedline.html',\n",
       " '/lyrics/billyjoel/thetimestheyareachangin.html',\n",
       " '/lyrics/billyjoel/theserhinestonedays.html',\n",
       " '/lyrics/billyjoel/tomakeyoufeelmylove.html',\n",
       " '/lyrics/billyjoel/whenyouwishuponastar.html',\n",
       " '/lyrics/billyjoel/wherewereyouonourweddingday.html',\n",
       " '/lyrics/billyjoel/whyshouldiworry.html',\n",
       " '/lyrics/billyjoel/youpickedarealbadtime.html',\n",
       " '/lyrics/billyjoel/youreonlyhumansecondwind.html']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_pages[artist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "cell_id": "02c22f486c404778b6eac658b4ebf41d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "a5b232cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyrric created.\n"
     ]
    }
   ],
   "source": [
    "# Make the lyrics folder here. If you'd like to practice your programming, add functionality \n",
    "# that checks to see if the folder exists. If it does, then use shutil.rmtree to remove it and create a new one.\n",
    "\n",
    "\n",
    "\n",
    "folder_name2 = \"lyrics\"\n",
    "\n",
    "#If you'd like to practice your programming, add functionality \n",
    "# that checks to see if the folder exists. If it does, then \"unlink\" it. Then create a new one.\n",
    "\n",
    "if os.path.exists(folder_name2):\n",
    "    os.rmdir(folder_name2)\n",
    "    print(f\"{folder_name2} removed.\")\n",
    "\n",
    "os.mkdir(folder_name2)\n",
    "print(f\"{folder_name2} created.\")\n",
    "\n",
    "if not os.path.isdir(\"lyrics\") : \n",
    "    shutil.rmtree(\"lyrics/\")\n",
    "    os.mkdir(\"Lyrric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_stub = \"https://www.azlyrics.com\" \n",
    "start = time.time()\n",
    "\n",
    "total_pages = 0 \n",
    "\n",
    "for artist in lyrics_pages :\n",
    "\n",
    "    # Use this space to carry out the following steps: \n",
    "    \n",
    "    # 1. Build a subfolder for the artist\n",
    "    artist_folder = os.path.join(\"lyrics\", artist)\n",
    "    if not os.path.exists(artist_folder):\n",
    "        os.makedirs(artist_folder)\n",
    "    \n",
    "    # 2. Iterate over the lyrics pages\n",
    "    for lyrics_page in lyrics_pages[artist]:\n",
    "    # 3. Request the lyrics page.\n",
    "        r = requests.get(\"https://www.azlyrics.com\" + lyrics_page)\n",
    "        #r = requests.get(url_stub + lyrics_page)\n",
    "        time.sleep(5 + 10*random.random())\n",
    "    # 4. Extract the title and lyrics from the page.\n",
    "        BSoup = BeautifulSoup(r.content, 'html.parser')\n",
    "        div = BSoup.find(\"div\", {\"class\": \"col-xs-12 col-lg-8 text-center\"})\n",
    "        t = div.find_all('b')\n",
    "        l = div.find_all('br')\n",
    "        title = t[1].text\n",
    "        #lyrics = l[1].text\n",
    "        lyrics = BSoup.find(class_= \"col-xs-12 col-lg-8 text-center\").text\n",
    "    \n",
    "    # 5. Write out the title, two returns ('\\n'), and the lyrics. Use `generate_filename_from_url` to generate the filename. \n",
    "        filename = generate_filename_from_link(lyrics_page)\n",
    "        with open(os.path.join(artist_folder, filename), \"w\") as f:\n",
    "            f.write(title + '\\n')\n",
    "            f.write(lyrics)\n",
    "            total_pages +=1 \n",
    "           \n",
    "    \n",
    "    \n",
    "    # Remember to pull at least 20 songs per artist. It may be fun to pull all the songs for the artist\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "cell_id": "e4ebdf4104ff4313a9997431cb78ade5",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "4d0424c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total run time was 0.69 hours.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total run time was {round((time.time() - start)/3600,2)} hours.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b733fdbd389445ecbe2d2adf4e22f98f",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "This assignment asks you to pull data from the Twitter API and scrape www.AZLyrics.com.  After you have finished the above sections , run all the cells in this notebook. Print this to PDF and submit it, per the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "cell_id": "d7db1085254e4b17b57e24545c705327",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1673853948807,
    "source_hash": "cd2d824c"
   },
   "outputs": [],
   "source": [
    "# Simple word extractor from Peter Norvig: https://norvig.com/spell-correct.html\n",
    "def words(text): \n",
    "    return re.findall(r'\\w+', text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "92c5310c21ab4a398764897d7eaa55b0",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---\n",
    "\n",
    "## Checking Twitter Data\n",
    "\n",
    "The output from your Twitter API pull should be two files per artist, stored in files with formats like `cher_followers.txt` (a list of all follower IDs you pulled) and `cher_followers_data.txt`. These files should be in a folder named `twitter` within the repository directory. This code summarizes the information at a high level to help the instructor evaluate your work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "cell_id": "1930ed296ae64b34b9fa2a15f6257594",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6,
    "execution_start": 1673853955045,
    "source_hash": "3a5bb533"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We see two artist handles: omarapollo and billyjoel.\n"
     ]
    }
   ],
   "source": [
    "twitter_files = os.listdir(\"twitter\")\n",
    "twitter_files = [f for f in twitter_files if f != \".DS_Store\"]\n",
    "artist_handles = list(set([name.split(\"_\")[0] for name in twitter_files]))\n",
    "\n",
    "print(f\"We see two artist handles: {artist_handles[0]} and {artist_handles[1]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "cell_id": "2cfa0fd8215e4d7d8a1593030378d1c6",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1673856316955,
    "source_hash": "3fda430b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We see 1000 in your follower file for omarapollo, assuming a header row.\n",
      "In the follower data file (omarapollo_follower_data1.txt) for omarapollo, we have these columns:\n",
      "          screen_name                                             name                   id                        location  followers_count  friends_count                                                                                                                                                       description\n",
      "\n",
      "We have 2000 data rows for omarapollo in the follower data file.\n",
      "For omarapollo we have 0 unique locations.\n",
      "For omarapollo we have 0 words in the descriptions.\n",
      "Here are the five most common words:\n",
      "[]\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "We see 1000 in your follower file for billyjoel, assuming a header row.\n",
      "In the follower data file (billyjoel_follower_data1.txt) for billyjoel, we have these columns:\n",
      "         screen_name                                           name                   id                        location  followers_count  friends_count                                                                                                                                                       description\n",
      "\n",
      "We have 1000 data rows for billyjoel in the follower data file.\n",
      "For billyjoel we have 0 unique locations.\n",
      "For billyjoel we have 0 words in the descriptions.\n",
      "Here are the five most common words:\n",
      "[]\n",
      "\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for artist in artist_handles :\n",
    "    follower_file = artist + \"_follower1.txt\"\n",
    "    follower_data_file = artist + \"_follower_data1.txt\"\n",
    "    \n",
    "    ids = open(\"twitter/\" + follower_file,'r').readlines()\n",
    "    \n",
    "    print(f\"We see {len(ids)-1} in your follower file for {artist}, assuming a header row.\")\n",
    "    \n",
    "    with open(\"twitter/\" + follower_data_file,'r') as infile :\n",
    "        \n",
    "        # check the headers\n",
    "        headers = infile.readline().split(\"\\t\")\n",
    "        \n",
    "        print(f\"In the follower data file ({follower_data_file}) for {artist}, we have these columns:\")\n",
    "        print(\" : \".join(headers))\n",
    "        \n",
    "        description_words = []\n",
    "        locations = set()\n",
    "        \n",
    "    \n",
    "        for idx, line in enumerate(infile.readlines()) :\n",
    "            line = line.strip(\"\\n\").split(\"\\t\")\n",
    "            \n",
    "            try : \n",
    "                locations.add(line[3])            \n",
    "                description_words.extend(words(line[6]))\n",
    "            except :\n",
    "                pass\n",
    "    \n",
    "        \n",
    "\n",
    "        print(f\"We have {idx+1} data rows for {artist} in the follower data file.\")\n",
    "\n",
    "        print(f\"For {artist} we have {len(locations)} unique locations.\")\n",
    "\n",
    "        print(f\"For {artist} we have {len(description_words)} words in the descriptions.\")\n",
    "        print(\"Here are the five most common words:\")\n",
    "        print(Counter(description_words).most_common(5))\n",
    "\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"-\"*40)\n",
    "        print(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: In the text file, there is words under descriptions and actual locations for the location header. These files will be provided along with the submission.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "19ba9d80869f4fde9cd9d36c47d5400d",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Checking Lyrics \n",
    "\n",
    "The output from your lyrics scrape should be stored in files located in this path from the directory:\n",
    "`/lyrics/[Artist Name]/[filename from URL]`. This code summarizes the information at a high level to help the instructor evaluate your work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "cell_id": "0126afe8f9ac470ea6166181d507da5f",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "7c366b86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Omar Apollo we have 56 files.\n",
      "For Omar Apollo we have roughly 21730 words, 2171 are unique.\n",
      "For billyjoel we have 149 files.\n",
      "For billyjoel we have roughly 49505 words, 3664 are unique.\n"
     ]
    }
   ],
   "source": [
    "artist_folders = os.listdir(\"lyrics/\")\n",
    "artist_folders = [f for f in artist_folders if os.path.isdir(\"lyrics/\" + f)]\n",
    "\n",
    "for artist in artist_folders : \n",
    "    artist_files = os.listdir(\"lyrics/\" + artist)\n",
    "    artist_files = [f for f in artist_files if 'txt' in f or 'csv' in f or 'tsv' in f]\n",
    "\n",
    "    print(f\"For {artist} we have {len(artist_files)} files.\")\n",
    "\n",
    "    artist_words = []\n",
    "\n",
    "    for f_name in artist_files : \n",
    "        with open(\"lyrics/\" + artist + \"/\" + f_name) as infile : \n",
    "            artist_words.extend(words(infile.read()))\n",
    "\n",
    "            \n",
    "    print(f\"For {artist} we have roughly {len(artist_words)} words, {len(set(artist_words))} are unique.\")\n"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "a178f6df132b4b6fbcf4b8166a67f7c6",
  "deepnote_persisted_session": {
   "createdAt": "2023-01-12T01:00:22.805Z"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
